---
title: "Naive dictionary approach"
output: html_notebook
---

```{r setup}
library(dplyr)
library(tidytext)
library(tm)
library(sentimentr)
library(ggplot2)
library(foreach)
library(doParallel)

source("../utils/kaggle_score.R")
```


Load the data

```{r}
train_set <- read.csv("../data/train.csv", stringsAsFactors = FALSE)
test_set <- read.csv("../data/test.csv", stringsAsFactors = FALSE)

category_names <- names(train_set[, 3:8])

head(train_set)
```

Create corpus and document term matrix

```{r}
train_corpus <- SimpleCorpus(VectorSource(train_set$comment_text),
                             control = list(language = "en"))

train_dtm <- DocumentTermMatrix(train_corpus, control = list(tokenize = "words",
                                                             tolower = TRUE,
                                                             removePunctuation = TRUE,
                                                             removeNumbers = TRUE,
                                                             stopwords = TRUE))

train_dtm <- train_dtm[, !colnames(train_dtm) %in% lexicon::function_words]

inspect(train_dtm)
```

Create a list of terms common to each document type

```{r}
# First tidy the DTM
train_t <- tidy(train_dtm)

# Add good comments category
train_set$good_comments <- ifelse(rowSums(train_set[, 3:8]) > 0, 0L, 1L)

# Which documents are classified as good, toxic, severe toxic, etc.
comment_types <- lapply(train_set[, 3:9], function(x) (1:length(x))[x == 1])

# Summarise term frequency for each type of comment
train_freqs <- lapply(comment_types, function(x) { 
  filter(train_t, document %in% x) %>%
  group_by(term) %>%
  summarise(count = sum(count))
})

# Add column with document name (toxic, severe toxic etc.)
train_freqs <- lapply(names(train_freqs), function(x) mutate(train_freqs[[x]], document = x))

# Cast into DTM with summarised document types
doc_types <- cast_dtm(bind_rows(train_freqs), document, term, count)

inspect(doc_types)
```

Create custom sentiment dictionary for each comment type 

```{r}
doc_types <- as.TermDocumentMatrix(doc_types)

bad_dicts <- lapply(category_names, function(x) {
  toxic.freq <- as.matrix(doc_types[, c("good_comments", x)])
  
  toxic.freq[, 1] <- toxic.freq[, 1] / sum(toxic.freq[, 1])
  toxic.freq[, 2] <- toxic.freq[, 2] / sum(toxic.freq[, 2])
  rmeans <- rowMeans(toxic.freq)
  toxic.freq[, 1] <- toxic.freq[, 1] - rmeans
  toxic.freq[, 2] <- toxic.freq[, 2] - rmeans
  
  toxic.dict <- tibble::rownames_to_column(as.data.frame(toxic.freq[, 1]), "term")
  names(toxic.dict)[2] <- "score"
  toxic.dict$score <- toxic.dict$score * (-1 / min(toxic.dict$score))
  res <- suppressWarnings(as_key(toxic.dict))
  res
})

names(bad_dicts) <- category_names
```

Run sentiment analysis on the dataset (30k comments)

```{r}
comments_text <- get_sentences(train_set$comment_text[1:30000])
comment_sentiments <- lapply(category_names, function(x) {
  -sentiment_by(comments_text, polarity_dt = bad_dicts[[x]])$ave_sentiment
})

names(comment_sentiments) <- category_names
comment_sentiments <- as.data.frame(comment_sentiments)
```

Now let's check how the initial 30k comments were classified

```{r}
kaggle_score(comment_sentiments, train_set[1:30000, 3:8])
```

Initial AUC of 0.91

# Scoring the test set

Create a parallel cluster

```{r}
cl <- makeCluster(3) # memory requirements go up quite a bit with more (each one requires ~4-5GB)
registerDoParallel(cl)
```

Calculate scores for each test set

```{r}
test_texts <- get_sentences(test_set$comment_text)
test_sentiments <- foreach(category=category_names) %dopar% {
  -sentimentr::sentiment_by(test_texts, polarity_dt = bad_dicts[[category]])$ave_sentiment
}

names(test_sentiments) <- category_names
test_sentiments <- as.data.frame(test_sentiments)

head(test_sentiments)
```

Now stop the cluster

```{r}
stopCluster(cl)
```

Rescale sentiment scores into probabilities [0, 1] and add id information

```{r}
test_submission <- sapply(test_sentiments, function(x) {
  minx <- min(x)
  maxx <- max(x)
  rangex <- maxx - minx
  return((x - minx) / rangex)
})
test_submission <- as.data.frame(test_submission)
test_submission$id <- test_set$id
test_submission <- test_submission[, c("id", "toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate")]
write.csv(test_submission, row.names = FALSE, file = "../submissions/sentiment_pb.csv")
```

And submitted to kaggle with a score of 0.9083
